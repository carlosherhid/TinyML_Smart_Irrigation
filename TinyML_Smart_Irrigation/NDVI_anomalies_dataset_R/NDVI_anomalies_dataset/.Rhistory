times = 1) #Genera solamente una partici칩n 80/20
str(arroz.TrainIdx.80) #Como podemos ver, es una matriz de una columna
arroz.Datos.Train<-datos[arroz.TrainIdx.80,]
arroz.Datos.Test<-datos[-arroz.TrainIdx.80,]
pca_result <- prcomp(arroz.Datos.Train[0:106], scale = TRUE)
modelo.rangerGuardado <- readRDS(file="ranger2.rds")
matriztest <- as.matrix(arroz.Datos.Test[0:106])
matriztest
predictores_test_pca <- matriztest %*% pca_result$rotation
str(predictores_test_pca)
View(predictores_test_pca)
prediccion_pca <-predict(modelo.rangerGuardado,
predictores_test_pca)
prediccion_pca
options(max.print=999999)
prediccion_pca
prediccion_pca <-predict(modelo.rangerGuardado,
matriztest)
View(pca_result)
predictores_Def <- pca_result$x[,0:6]
View(predictores_Def)
View(predictores_test_pca)
View(predictores_Def)
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(caret)
library(mlbench)
library(openssl)
library(ranger)
library("readxl")
library("mlbench")
library("caret")
datos <- read_excel("Rice_MSC_Dataset.xlsx") #leemos los datos del excel
datos <- na.omit(datos) #Eliminamos los datos que sean NA
arroz.Var.Salida.Usada = c("CLASS")
arroz.Vars.Entrada.Usadas = setdiff(names(datos),arroz.Var.Salida.Usada)
set.seed(1234)
arroz.TrainIdx.80<- createDataPartition(datos[[arroz.Var.Salida.Usada]],
p=0.8, #Genera un 80% para train, 20% para test
list = FALSE, #Dame los resultados en una matriz
times = 1) #Genera solamente una partici칩n 80/20
str(arroz.TrainIdx.80) #Como podemos ver, es una matriz de una columna
arroz.Datos.Train<-datos[arroz.TrainIdx.80,]
arroz.Datos.Test<-datos[-arroz.TrainIdx.80,]
pca_result <- prcomp(arroz.Datos.Train[0:106], scale = TRUE)
(VE <- pca_result$sdev^2)
PVE <- VE / sum(VE)
round(PVE, 2)
cumsum(PVE)
predictores_Def <- pca_result$x[,0:6]
arroz.trainCtrl <- trainControl(## Crosvalidaci칩n de 5 pliegues
method = "repeatedcv",
number = 5,
## repetido 3 veces
repeats = 3)
rangerGrid = expand.grid(mtry = c(3,4,5),
splitrule= c("gini","extratrees"),
min.node.size = 1)
rangerGrid1 = expand.grid(mtry = c(3,4,5),
splitrule= c("gini","extratrees"),
min.node.size = 1)
print(rangerGrid1)
system.time(modelo.ranger2 <- train(predictores_rfe_def, arroz.Datos.Train[[arroz.Var.Salida.Usada]], method='ranger', trControl = arroz.trainCtrl, tuneGrid = rangerGrid))
rfProfilesaved <- readRDS(file="rfProfile.rds")
pred_rfe <- c("StdDevB", "StdDevCb", "SHAPEFACTOR_3", "COMPACTNESS", "ROUNDNESS")
predictores_rfe_def<-arroz.Datos.Train[,pred_rfe]
system.time(modelo.ranger2 <- train(predictores_rfe_def, arroz.Datos.Train[[arroz.Var.Salida.Usada]], method='ranger', trControl = arroz.trainCtrl, tuneGrid = rangerGrid))
saveRDS(modelo.ranger2, "ranger3.rds")
rfGrid1 = expand.grid(mtry = c(1,2,3,4,5))
system.time(modelo.rf2 <- train(predictores_rfe_def, arroz.Datos.Train[[arroz.Var.Salida.Usada]], method='rf', trControl = arroz.trainCtrl, tuneGrid = rfGrid))
system.time(modelo.rf2 <- train(predictores_rfe_def, arroz.Datos.Train[[arroz.Var.Salida.Usada]], method='rf', trControl = arroz.trainCtrl, tuneGrid = rfGrid1))
saveRDS(modelo.rf2, "rf3.rds")
mlpKerasDropoutGrid1 = expand.grid(size = c(1,2,3,4,5),
dropout = c(0.0,0.1,0.2),
lr = c(1.0, 0.7, 0.5, 0.2),
rho = c(0.9, 0.95),
batch_size = 19998,
activation = "relu",
decay = 0)
system.time(modelo.mlpKerasDropout3 <- train(predictores_rfe_def, arroz.Datos.Train[[arroz.Var.Salida.Usada]], method='mlpKerasDropout', trControl = arroz.trainCtrl, tuneGrid = mlpKerasDropoutGrid1))
```{r}
saveRDS(modelo.mlpKerasDropout3, "mlpKerasDropout2.rds")
modelo.mlpKerasGuardado2 <- readRDS(file="mlpKerasDropout2.rds")
modelo.mlpKerasGuardado2
knitr::opts_chunk$set(echo = TRUE)
scaled_df <- apply(arroz.Datos.Test[0:106], 2, scale)
apply(scaled_df, 2, mean)
apply(scaled_df, 2, var)
matriztest <- as.matrix(arroz.Datos.Test[0:106])
predictores_test_pca <- matriztest %*% pca_result$rotation[,0:6]
prediccion_pca <-predict(modelo.rangerGuardado,
predictores_test_pca)
modelo.rangerGuardado <- readRDS(file="ranger2.rds")
prediccion_pca <-predict(modelo.rangerGuardado,
predictores_test_pca)
MLmetrics::Accuracy(prediccion_pca,  arroz.Datos.Test[[107]])
arroz.Datos.Test
matriztest <- as.matrix(scaled_df)
predictores_test_pca <- matriztest %*% pca_result$rotation[,0:6]
prediccion_pca <-predict(modelo.rangerGuardado,
predictores_test_pca)
MLmetrics::Accuracy(prediccion_pca,  arroz.Datos.Test[[107]])
modelo.rangerGuardado2 <- readRDS(file="ranger3.rds")
modelo.rfGuardado2 <-readRDS(file="rf3.rds")
modelo.mlpKerasGuardado2 <- readRDS(file="mlpKerasDropout2.rds")
modelo.rangerGuardado2
modelo.rfGuardado2
modelo.mlpKerasGuardado2
modelo.rangerGuardado2
modelo.rfGuardado2
modelo.mlpKerasGuardado2
MLmetrics::Accuracy(prediccion_pca,  arroz.Datos.Test[[107]])
prediccion_rfe <-predict(modelo.rfGuardado2,
arroz.Datos.Test[0:106])
MLmetrics::Accuracy(prediccion_rfe,  arroz.Datos.Test[[107]])
source(file.path("code","include.R"), encoding = "UTF-8")
install.packages("shiny")
install.packages("shiny")
shiny::runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/MemoriaODINsCarlosHH/RiegoInteligenteClustering/RiegoInteligenteClustering')
install.packages("elasticserachr")
runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/MemoriaODINsCarlosHH/RiegoInteligenteClustering/RiegoInteligenteClustering')
runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/MemoriaODINsCarlosHH/RiegoInteligenteClustering/RiegoInteligenteClustering')
install.packages(c("cachem", "caret", "classInt", "cli", "clv", "colorspace", "curl", "data.table", "digest", "dplyr", "e1071", "fansi", "fastmap", "forecast", "gower", "httpuv", "ipred", "later", "lattice", "lubridate", "plyr", "pROC", "prodlim", "purrr", "Rcpp", "rgl", "sf", "stringi", "tibble", "tidyr", "timechange", "tseries", "units", "utf8", "xfun", "xts", "yaml", "zoo"))
install.packages(c("cachem", "caret", "classInt", "cli", "clv", "colorspace", "curl", "data.table", "digest", "dplyr", "e1071", "fansi", "fastmap", "forecast", "gower", "httpuv", "ipred", "later", "lattice", "lubridate", "plyr", "pROC", "prodlim", "purrr", "Rcpp", "rgl", "sf", "stringi", "tibble", "tidyr", "timechange", "tseries", "units", "utf8", "xfun", "xts", "yaml", "zoo"))
install.packages(c("cachem", "caret", "classInt", "cli", "clv", "colorspace", "curl", "data.table", "digest", "dplyr", "e1071", "fansi", "fastmap", "forecast", "gower", "httpuv", "ipred", "later", "lattice", "lubridate", "plyr", "pROC", "prodlim", "purrr", "Rcpp", "rgl", "sf", "stringi", "tibble", "tidyr", "timechange", "tseries", "units", "utf8", "xfun", "xts", "yaml", "zoo"))
install.packages(c("cachem", "caret", "classInt", "cli", "clv", "colorspace", "curl", "data.table", "digest", "dplyr", "e1071", "fansi", "fastmap", "forecast", "gower", "httpuv", "ipred", "later", "lattice", "lubridate", "plyr", "pROC", "prodlim", "purrr", "Rcpp", "rgl", "sf", "stringi", "tibble", "tidyr", "timechange", "tseries", "units", "utf8", "xfun", "xts", "yaml", "zoo"))
shiny::runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/RiegoInteligente2/RiegoInteligente/RiegoInteligenteC')
install.packages("elasticsearchr")
runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/RiegoInteligenteElastic')
runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/RiegoInteligenteElastic')
runApp('C:/Users/carlosherhid/Desktop/Tercero/practicas externas/odin/RiegoInteligenteClustering')
setwd("C:/Users/carlosherhid/Desktop/Cuarto/TinyML_Smart_Irrigation/TinyML_Smart_Irrigation/NDVI_anomalies_dataset_R/NDVI_anomalies_dataset")
#Script for preparing humidity and temperature data along with NDVI to create a dataset that relates these three variables and determine if humidity and temperature are anomalous with respect
#to certain NDVI values that we will define as anomalies (very low or high NDVI values for a specific park).
#Author: Carlos Hern치ndez Hidalgo
source(file.path("code","include.R"), encoding = "UTF-8")
library(dplyr)
library(knitr)
library(plyr)
library(readr)
################################################
ind = dflt$ind   #Measurement index, in this case, NDVI.
mask = dflt$mask #Mask, in this case, "scl_7_8_9".
agg = dflt$agg #"mean"
serie = dflt$serie #type of series, "Verdor"
pkg = dflt$pkg #packet used for clustering, "TSClust"
ini = as.Date("2018-01-13") #dflt$ini #Start date for filtering to gather the data for which we want to perform clustering.
fin = as.Date("2022-02-20") #dflt$fin #End date for filtering the data on which we intend to apply clustering.
aggl = "complete" #Used to execute the clustering.
################################################
#We gather and prepare all humidity and temperature data from different weather stations.
################################################
ini_mediciones = as.Date("2018-01-6")  #Start date for filtering to obtain humidity and temperature data (ten days prior to get the 10 measurements before the first one).
fin_mediciones = as.Date("2021-03-31") #End date for filtering the humidity and temperature data.
################################################
# CA42 Station
################################################
dat_CA42 <- read_csv2(file.path("datos","TempHum","CA42.txt"),col_types = cols(.default = "c"))
#We remove the columns that are not needed.
aux <- dat_CA42 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were in "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_CA42 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#CA91 station
################################################
dat_CA91 <- read_csv2(file.path("datos","TempHum","CA91.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_CA91 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_CA91 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MO12 station
################################################
dat_MO12 <- read_csv2(file.path("datos","TempHum","MO12.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MO12 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MO12 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MU21 station
################################################
dat_MU21 <- read_csv2(file.path("datos","TempHum","MU21.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MU21 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values)
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MU21 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MU62 station
################################################
dat_MU62 <- read_csv2(file.path("datos","TempHum","MU62.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MU62 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values)
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MU62 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#We prepare the contracts.
################################################
contratos <- dflt$contratos("ConSeries", ind, mask, agg)
################################################
#I obtain the contracts and connections classified according to the nearest weather station.
################################################
estaciones <- DatGeo$leerEstMet() #The weather stations with their locations.
dflt$eemm #The names of the weather stations.
#We obtain the connection number, contract, and nearest or main weather station.
sg <- DatGeo$asignarEstMet() %>%
dplyr::select(Acometida, Contrato, Principal)
contrato_ca42 <- sg%>%
dplyr::filter(Principal == dflt$eemm[1]) %>% na.omit()
contrato_ca42 <- data.frame(contrato_ca42[[1]],contrato_ca42[[2]], contrato_ca42[[3]]) #We removed the "geometry" column.
colnames(contrato_ca42) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_CA91 <- sg%>%
dplyr::filter(Principal == dflt$eemm[2]) %>% na.omit()
contrato_CA91 <- data.frame(contrato_CA91[[1]],contrato_CA91[[2]], contrato_CA91[[3]]) #We removed the "geometry" column.
colnames(contrato_CA91) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MO12 <-sg%>%
dplyr::filter(Principal == dflt$eemm[3]) %>% na.omit()
contrato_MO12 <- data.frame(contrato_MO12[[1]],contrato_MO12[[2]], contrato_MO12[[3]]) #We removed the "geometry" column.
colnames(contrato_MO12) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MU21 <-sg%>%
dplyr::filter(Principal == dflt$eemm[4]) %>% na.omit()
contrato_MU21 <- data.frame(contrato_MU21[[1]],contrato_MU21[[2]], contrato_MU21[[3]]) #We removed the "geometry" column.
colnames(contrato_MU21) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MU62 <-sg%>%
dplyr::filter(Principal == dflt$eemm[5]) %>% na.omit()
contrato_MU62 <- data.frame(contrato_MU62[[1]],contrato_MU62[[2]], contrato_MU62[[3]]) #Eliminamos la columna "geometry"
colnames(contrato_MU62) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
################################################
#We prepare S, which will be the initial NDVI series.
#Each park is identified by a contract number.
#For each park, we have the following data:
#   -Fecha: Day on which the NDVI measurements were taken, measurements are taken every 5 days.
#   -NDVI.scl_7_8_9.Min: The minimum NDVI value for that day.
#   -NDVI.scl_7_8_9.Q1: The first quartile of the NDVI values.
#   -NDVI.scl_7_8_9.Median: The median of the NDVI values.
#   -NDVI.scl_7_8_9.Mean: The mean of the NDVI values.
#   -NDVI.scl_7_8_9.Q3: The third quartile of the NDVI values.
#   -NDVI.scl_7_8_9.Max: The maximum NDVI value for that day.
#   -NDVI.scl_7_8_9.n: The number of NDVI measurements taken on that day for that park.
################################################
S <- Verdor$readSumInd(contratos, ind, mask, ini = ini, fin = fin)
View(S)
Date <-
listDF2DF(S) %>%
dplyr::select(Contrato, Fecha, NDVI.scl_7_8_9.Mean) %>%
fil2col() %>% dplyr::select(Fecha)
library(tidyr)
#Script for preparing humidity and temperature data along with NDVI to create a dataset that relates these three variables and determine if humidity and temperature are anomalous with respect
#to certain NDVI values that we will define as anomalies (very low or high NDVI values for a specific park).
#Author: Carlos Hern치ndez Hidalgo
source(file.path("code","include.R"), encoding = "UTF-8")
library(dplyr)
library(knitr)
library(plyr)
library(readr)
################################################
ind = dflt$ind   #Measurement index, in this case, NDVI.
mask = dflt$mask #Mask, in this case, "scl_7_8_9".
agg = dflt$agg #"mean"
serie = dflt$serie #type of series, "Verdor"
pkg = dflt$pkg #packet used for clustering, "TSClust"
ini = as.Date("2018-01-13") #dflt$ini #Start date for filtering to gather the data for which we want to perform clustering.
fin = as.Date("2022-02-20") #dflt$fin #End date for filtering the data on which we intend to apply clustering.
aggl = "complete" #Used to execute the clustering.
################################################
#We gather and prepare all humidity and temperature data from different weather stations.
################################################
ini_mediciones = as.Date("2018-01-6")  #Start date for filtering to obtain humidity and temperature data (ten days prior to get the 10 measurements before the first one).
fin_mediciones = as.Date("2021-03-31") #End date for filtering the humidity and temperature data.
################################################
# CA42 Station
################################################
dat_CA42 <- read_csv2(file.path("datos","TempHum","CA42.txt"),col_types = cols(.default = "c"))
#We remove the columns that are not needed.
aux <- dat_CA42 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were in "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_CA42 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#CA91 station
################################################
dat_CA91 <- read_csv2(file.path("datos","TempHum","CA91.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_CA91 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_CA91 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MO12 station
################################################
dat_MO12 <- read_csv2(file.path("datos","TempHum","MO12.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MO12 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values).
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MO12 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MU21 station
################################################
dat_MU21 <- read_csv2(file.path("datos","TempHum","MU21.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MU21 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values)
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MU21 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#MU62 station
################################################
dat_MU62 <- read_csv2(file.path("datos","TempHum","MU62.txt"),col_types = cols(.default = "c"))
#We remove the columns that we don't need.
aux <- dat_MU62 %>%
dplyr::select(fecha,tmed,hrmed)
#We convert the temperature and humidity data to numerical values. They were initially in the form of "Name characters".
#We remove the names.
names(aux$tmed) <- NULL
names(aux$hrmed) <- NULL
#We need to convert characters to numbers.
aux$tmed <- as.numeric(aux$tmed)
aux$hrmed <- as.numeric(aux$hrmed)
aux$fecha <- as.Date(aux$fecha, format = "%d/%m/%y")
#We remove the rows that contain NAs (missing values)
aux <- na.omit(aux)
#We group by dates and calculate the mean of the measurements for each day.
aux1 <- ddply(aux, "fecha",colwise(mean))
#We keep only the dates for which we have NDVI data.
TH_MU62 <- aux1 %>%
dplyr::filter(fecha >= ini_mediciones) %>%
dplyr::filter(fecha <= fin_mediciones)
################################################
#We prepare the contracts.
################################################
contratos <- dflt$contratos("ConSeries", ind, mask, agg)
################################################
#I obtain the contracts and connections classified according to the nearest weather station.
################################################
estaciones <- DatGeo$leerEstMet() #The weather stations with their locations.
dflt$eemm #The names of the weather stations.
#We obtain the connection number, contract, and nearest or main weather station.
sg <- DatGeo$asignarEstMet() %>%
dplyr::select(Acometida, Contrato, Principal)
contrato_ca42 <- sg%>%
dplyr::filter(Principal == dflt$eemm[1]) %>% na.omit()
contrato_ca42 <- data.frame(contrato_ca42[[1]],contrato_ca42[[2]], contrato_ca42[[3]]) #We removed the "geometry" column.
colnames(contrato_ca42) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_CA91 <- sg%>%
dplyr::filter(Principal == dflt$eemm[2]) %>% na.omit()
contrato_CA91 <- data.frame(contrato_CA91[[1]],contrato_CA91[[2]], contrato_CA91[[3]]) #We removed the "geometry" column.
colnames(contrato_CA91) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MO12 <-sg%>%
dplyr::filter(Principal == dflt$eemm[3]) %>% na.omit()
contrato_MO12 <- data.frame(contrato_MO12[[1]],contrato_MO12[[2]], contrato_MO12[[3]]) #We removed the "geometry" column.
colnames(contrato_MO12) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MU21 <-sg%>%
dplyr::filter(Principal == dflt$eemm[4]) %>% na.omit()
contrato_MU21 <- data.frame(contrato_MU21[[1]],contrato_MU21[[2]], contrato_MU21[[3]]) #We removed the "geometry" column.
colnames(contrato_MU21) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
contrato_MU62 <-sg%>%
dplyr::filter(Principal == dflt$eemm[5]) %>% na.omit()
contrato_MU62 <- data.frame(contrato_MU62[[1]],contrato_MU62[[2]], contrato_MU62[[3]]) #Eliminamos la columna "geometry"
colnames(contrato_MU62) <- c("Acometida","Contrato","Estacion") #We changed the names of the columns.
################################################
#We prepare S, which will be the initial NDVI series.
#Each park is identified by a contract number.
#For each park, we have the following data:
#   -Fecha: Day on which the NDVI measurements were taken, measurements are taken every 5 days.
#   -NDVI.scl_7_8_9.Min: The minimum NDVI value for that day.
#   -NDVI.scl_7_8_9.Q1: The first quartile of the NDVI values.
#   -NDVI.scl_7_8_9.Median: The median of the NDVI values.
#   -NDVI.scl_7_8_9.Mean: The mean of the NDVI values.
#   -NDVI.scl_7_8_9.Q3: The third quartile of the NDVI values.
#   -NDVI.scl_7_8_9.Max: The maximum NDVI value for that day.
#   -NDVI.scl_7_8_9.n: The number of NDVI measurements taken on that day for that park.
################################################
S <- Verdor$readSumInd(contratos, ind, mask, ini = ini, fin = fin)
################################################
#We obtain the dates.
################################################
Date <-
listDF2DF(S) %>%
dplyr::select(Contrato, Fecha, NDVI.scl_7_8_9.Mean) %>%
fil2col() %>% dplyr::select(Fecha)
