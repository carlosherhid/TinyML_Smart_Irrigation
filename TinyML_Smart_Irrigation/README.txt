This GitHub repository contains the work done by Carlos Hernández Hidalgo and Aurora González Vidal. The objective was to create a comprehensive dataset that could aid in predicting NDVI anomalies. Following this, various machine learning models were trained using the dataset. These models will be optimized and quantized for implementation on small devices, such as sensors in fields or green parks, using the TinyML methodology.

-Model_Temp_Hum_NDVI_Python: This folder contains several Python scripts, each corresponding to a different type of machine learning model trained using the NDVI anomaly dataset.

	*graficos: In this folder we can find some graphics generated by running the python scripts of the different ML models.

	*"LinearRegressor_TempHumNDVI_model.py": A script that trains a Linear Regression model with the dataset generated that predicts NDVI anomalies using 	temperature, NDVI mean, humidity... It also generates a graphic of the 	prediction and accuracy achieved.

	*"RandomForest_TempHumNDVI_model.py": A script that trains a RandomForest model with the dataset generated that predicts NDVI anomalies using temperature, NDVI mean, humidity... It also generates a graphic of the prediction and 	accuracy achieved.

	*"NN_Model_TFLite_tinyML.py": A script that trains a Neural Network model 10 times with the dataset generated that predicts NDVI anomalies using temperature, 	NDVI mean, humidity... It also generates a graphic of the prediction 	and accuracy achieved in the best scenario. After that, the best model is converted in a 	TFLite model and optimized using quantization in order to reduce the size of the model considerably.

	*The .csv files are the different datasets used to train the models, one for each weather station.

-NDVI_anomalies_dataset_R: In this folder, several R scripts can be found. These scripts are used to generate a dataset comprising NDVI anomalies, humidity, NDVI means, temperature, and more. The resulting dataset will serve as the foundation for training the machine learning models.

	*The folders "code", "colorRamps", "datos", "datosTFG", "paramsTS" contains data and scripts used to gather diferent paramethers. It is important to say that 	in the folder /datos/TempHum we can find the temperature and humidity 	raw data that came from different weather stations.

	*The R script "weather_stations_locations.r" is used to gather the data and the location of each weather station we have.

	*The R script "Obtain_temp_hum_data.r" is used to gather the temperature and humidity data.

	*The file "TempHumNDVI.r" is a R script for preparing humidity and temperature data along with NDVI to create a dataset that relates these three variables 	and determine if humidity and temperature are anomalous with respect 	to certain NDVI values that we will define as anomalies (very low or high NDVI values 	for a specific park).

	*The .csv files are the datasets generated by running the script "TempHumNDVI.r", one file for each weather station.

-Steps followed to Run the models and generate the datasets:

	1. You have to run the script ".\NDVI_anomalies_dataset_R\TempHumNDVI.r" and after that the five datasets for each weather station will be generated, 5 .csv files.
	2. You have to move the 5 .csv files into the folder "./Model_Temp_Hum_NDVI_Python"
	3. After that, now you can run the python machine learning model that you want located in the folder "./Model_Temp_Hum_NDVI_Python", for example "NN_Model_TFLite_tinyML.py", that runs a Neural network model.